{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62797e5c",
   "metadata": {},
   "source": [
    "# Present a pseudo-code for a simple Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9480bde",
   "metadata": {},
   "source": [
    "```\n",
    "func buildDecisionTree(X, y):\n",
    "    if stopping_condition() == true:   # either reach max depth, min samples split, or pure class\n",
    "        return create_leaf_node(class = most_common_class(y))\n",
    "    best_impurity = infinity\n",
    "    best_feature = null\n",
    "    best_threshold = null\n",
    "    for each feature in X:\n",
    "        if feature is categorical:\n",
    "            impurity = sum (weighted (gini)-impurity of each category)\n",
    "            if impurity < best_impurity:\n",
    "                best_impurity = impurity\n",
    "                best_feature = feature\n",
    "                best_threshold = null   # no threshold for categorical feature\n",
    "        else:  # feature is numerical\n",
    "            for each threshold in midpoint of adjacent values in sorted(feature):\n",
    "                X_left, y_left = subset where feature < threshold\n",
    "                X_right, y_right = subset where feature >= threshold\n",
    "                impurity = sum(weighted (gini)-impurity of left and right subsets)\n",
    "                if impurity < best_impurity:\n",
    "                    best_impurity = impurity\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "    node = DecisionNode(feature = best_feature, threshold = best_threshold)\n",
    "    if best_feature is categorical:\n",
    "        for each category in unique values of best_feature:\n",
    "            X_subset, y_subset = X[best_feature] == category, y[best_feature] == category\n",
    "            node.children[category] = buildDecisionTree(X_subset, y_subset)\n",
    "    else:  # best_feature is numerical\n",
    "        X_left, y_left = X[best_feature] < best_threshold, y[best_feature] < best_threshold\n",
    "        X_right, y_right = X[best_feature] >= best_threshold, y[best_feature] >= best_threshold\n",
    "        node.children[\"left\"] = buildDecisionTree(X_left, y_left)\n",
    "        node.children[\"right\"] = buildDecisionTree(X_right, y_right)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03340ecd",
   "metadata": {},
   "source": [
    "# Implement decision tree algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d7ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from main import handle_missing_values\n",
    "from decision_tree import DecisionTree\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "train_data = handle_missing_values(train_data)\n",
    "test_data = handle_missing_values(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d787c5",
   "metadata": {},
   "source": [
    "### Train model with `max_depth=10`, `min_sample_split=20`, and Gini index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac690a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8343109855348423\n",
      "Test Accuracy: 0.8304772434125668\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTree(max_depth=10, min_samples_split=20, criterion='gini')\n",
    "dt.fit(train_data.drop('income', axis=1), train_data['income'])\n",
    "print(\"Train Accuracy:\", dt.evaluate(train_data.drop('income', axis=1), train_data['income']))\n",
    "print(\"Test Accuracy:\", dt.evaluate(test_data.drop('income', axis=1), test_data['income']))\n",
    "# Take about 1m10s to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a4195f",
   "metadata": {},
   "source": [
    "### Same setup as above but use Information Gain as criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cef4fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.833881023310095\n",
      "Test Accuracy: 0.8301701369694736\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTree(max_depth=10, min_samples_split=20, criterion='entropy')\n",
    "dt.fit(train_data.drop('income', axis=1), train_data['income'])\n",
    "print(\"Train Accuracy:\", dt.evaluate(train_data.drop('income', axis=1), train_data['income']))\n",
    "print(\"Test Accuracy:\", dt.evaluate(test_data.drop('income', axis=1), test_data['income']))\n",
    "# Take about 1m30s to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844be17b",
   "metadata": {},
   "source": [
    "- Both criteria produce very similar results on both training and testing data.\n",
    "- One reason that could explain why the results are so similar because the size of the dataset which is sufficiently large enough\n",
    "- If training time matters, Gini usually slightly faster to compute than Entropy (because of log in Entropy)\n",
    "- The very similar results also mean the data features and tree configuration (except criterion) dominate the model performance; criterion choice has minimal impact for this specific case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc1efbc",
   "metadata": {},
   "source": [
    "### Plot accuracy graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ec202",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dict = {}\n",
    "for min_sample_split in range(5, 101, 5):\n",
    "    dt = DecisionTree(max_depth=10, min_samples_split=min_sample_split, criterion='entropy')\n",
    "    dt.fit(train_data.drop('income', axis=1), train_data['income'])\n",
    "    train_accuracy = dt.evaluate(train_data.drop('income', axis=1), train_data['income'])\n",
    "    test_accuracy = dt.evaluate(test_data.drop('income', axis=1), test_data['income'])\n",
    "    accuracy_dict[min_sample_split] = [train_accuracy, test_accuracy]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(accuracy_dict.keys()), [v[0] for v in accuracy_dict.values()], label='Train Accuracy')\n",
    "plt.plot(list(accuracy_dict.keys()), [v[1] for v in accuracy_dict.values()], label='Test Accuracy')\n",
    "plt.xlabel('min_samples_split')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs min_samples_split (max_depth=10, criterion=entropy)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c79840",
   "metadata": {},
   "source": [
    "### Why training data accuracy is not 100%?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9542289",
   "metadata": {},
   "source": [
    "- There are many factors that can potentially contribut to why the accuracy of training data is not 100%\n",
    "- Real-world data is messy. This could include overlap classes (same feature but different class), hardware/software/human makes error while collecting the data \n",
    "- The pattern of the data is more complex than the model, thus the model cannot perfectly seperate the patterns\n",
    "- Sometimes all the available features still cannot explain all the variation in the target class.\n",
    "\n",
    "**Is there a way to get 100% accuracy on training dataset?**\n",
    "- We ca get 100% accuracy by overfitting the training data\n",
    "\n",
    "**What parameters should you change to get a tree with perfect training accuracy?**\n",
    "- Set `max_depth` to a very large value and `min_sample_split` to 1. This give the tree to grow as deep and complex as possible so it keeps splitting until each leaf node contains single class.\n",
    "\n",
    "**How would that affect the accuracy of the test data?**\n",
    "- The tree is essentially memorizing the training data, not learning the underlying patterns of the sample data\n",
    "- It will likely perform poorly on new, unseen test data because it cannot generalize \n",
    "\n",
    "**Explain if such a classification model has a high variance or bias**\n",
    "\n",
    "Such decision tree model (unrestricted decision tree) has:\n",
    "- Low bias: the model fit the training data extremely well, making as few assumptions as possible can capture every subtle details of the training data\n",
    "- High variance: such model is highly sensitive to small changes in the training data. E.g. If I provide different training samples or alter the data, the model's predictions and decisions can change dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138bf73d",
   "metadata": {},
   "source": [
    "### Train 16 decision trees and plot graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3952b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dict = {}\n",
    "for depth in range(1, 17):\n",
    "    dt = DecisionTree(max_depth=depth, min_samples_split=10, criterion='gini')\n",
    "    dt.fit(train_data.drop('income', axis=1), train_data['income'])\n",
    "    train_accuracy = dt.evaluate(train_data.drop('income', axis=1), train_data['income'])\n",
    "    test_accuracy = dt.evaluate(test_data.drop('income', axis=1), test_data['income'])\n",
    "    accuracy_dict[depth] = [train_accuracy, test_accuracy]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(accuracy_dict.keys()), [v[0] for v in accuracy_dict.values()], label='Train Accuracy')\n",
    "plt.plot(list(accuracy_dict.keys()), [v[1] for v in accuracy_dict.values()], label='Test Accuracy')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs max_depth (min_samples_split=10, criterion=gini)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
